
PYSPARK_APP_HOME=./src/spark
_PIP_ADDITIONAL_REQUIREMENTS=''

# Airflow Core
AIRFLOW__CORE__FERNET_KEY=UKMzEm3yIuFYEq1y3-2FxPNWSVwRASpahmQ9kQfEr8E=
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW_UID=0
AIRFLOW_PROJ_DIR=./src

# Backend DB
AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False

# Airflow Init
_AIRFLOW_DB_UPGRADE=True
_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow

# AIRFLOW_UID=0 # uncomment in case of running this from linux machine

# We set the spark connection as env variable instead of setting from the UI
AIRFLOW_CONN_SPARK_DEFAULT='spark://local%5B%2A%5D' # translates to spark://local[*]
AIRFLOW_CONN_KAFKA_DEFAULT='Kafka://localhost:9092'
